{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from models.super_alexnet import Super_AlexNet\n",
    "from generate_filter import Kernel\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#device : GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "    \"val\": transforms.Compose([transforms.Resize((224, 224)),  # cannot 224, must (224, 224)\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
    "data_root = os.getcwd()\n",
    "image_path = data_root + \"/data/\"  # flower data set path\n",
    "train_dataset = datasets.ImageFolder(root=image_path + \"/train\",\n",
    "                                     transform=data_transform[\"train\"])\n",
    "train_num = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_list = train_dataset.class_to_idx\n",
    "cla_dict = dict((val, key) for key, val in cd_list.items())\n",
    "# write dict into json file\n",
    "json_str = json.dumps(cla_dict, indent=4)\n",
    "with open('class_indices.json', 'w') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) <class 'torch.Tensor'>\n",
      "tensor(8) 8 <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(root=image_path + \"/val\",\n",
    "                                        transform=data_transform[\"val\"])\n",
    "val_num = len(validate_dataset)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                              batch_size=batch_size, shuffle=True,\n",
    "                                              num_workers=0)\n",
    "\n",
    "test_data_iter = iter(validate_loader)\n",
    "test_image, test_label = test_data_iter.next()\n",
    "print(test_image[0].size(),type(test_image[0]))\n",
    "print(test_label[0],test_label[0].item(),type(test_label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = Kernel()().to(device)\n",
    "\n",
    "net = Super_AlexNet(kernels, num_classes=9, init_weights=False)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0002)\n",
    "\n",
    "save_path = './Super_AlexNet.pth'\n",
    "\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 100%[**************************************************->]1.533\n",
      "111.88042889999998\n",
      "[epoch 1] train_loss: 1.903  test_accuracy: 0.328\n",
      "train loss: 100%[**************************************************->]1.177\n",
      "113.2272126\n",
      "[epoch 2] train_loss: 1.653  test_accuracy: 0.409\n",
      "train loss: 100%[**************************************************->]1.823\n",
      "113.05116939999999\n",
      "[epoch 3] train_loss: 1.519  test_accuracy: 0.352\n",
      "train loss: 100%[**************************************************->]1.559\n",
      "112.37383089999997\n",
      "[epoch 4] train_loss: 1.315  test_accuracy: 0.463\n",
      "train loss: 100%[**************************************************->]1.515\n",
      "112.2781185\n",
      "[epoch 5] train_loss: 1.177  test_accuracy: 0.546\n",
      "train loss: 100%[**************************************************->]0.413\n",
      "112.13383969999995\n",
      "[epoch 6] train_loss: 1.161  test_accuracy: 0.506\n",
      "train loss: 100%[**************************************************->]1.056\n",
      "112.19054730000005\n",
      "[epoch 7] train_loss: 1.076  test_accuracy: 0.587\n",
      "train loss: 100%[**************************************************->]0.566\n",
      "111.9162619\n",
      "[epoch 8] train_loss: 1.029  test_accuracy: 0.626\n",
      "train loss: 100%[**************************************************->]0.716\n",
      "111.86713959999997\n",
      "[epoch 9] train_loss: 0.988  test_accuracy: 0.576\n",
      "train loss: 100%[**************************************************->]1.330\n",
      "111.95532879999996\n",
      "[epoch 10] train_loss: 0.965  test_accuracy: 0.630\n",
      "train loss: 100%[**************************************************->]0.346\n",
      "112.06389939999985\n",
      "[epoch 11] train_loss: 0.947  test_accuracy: 0.669\n",
      "train loss: 100%[**************************************************->]0.401\n",
      "112.41759790000015\n",
      "[epoch 12] train_loss: 0.878  test_accuracy: 0.681\n",
      "train loss: 100%[**************************************************->]0.476\n",
      "112.23716569999988\n",
      "[epoch 13] train_loss: 0.918  test_accuracy: 0.641\n",
      "train loss: 100%[**************************************************->]0.629\n",
      "112.35611470000003\n",
      "[epoch 14] train_loss: 0.844  test_accuracy: 0.683\n",
      "train loss: 100%[**************************************************->]0.435\n",
      "112.29810020000014\n",
      "[epoch 15] train_loss: 0.874  test_accuracy: 0.639\n",
      "train loss: 100%[**************************************************->]0.654\n",
      "112.33105849999993\n",
      "[epoch 16] train_loss: 0.865  test_accuracy: 0.669\n",
      "train loss: 100%[**************************************************->]1.131\n",
      "112.17170050000004\n",
      "[epoch 17] train_loss: 0.825  test_accuracy: 0.724\n",
      "train loss: 100%[**************************************************->]1.332\n",
      "111.76611150000008\n",
      "[epoch 18] train_loss: 0.846  test_accuracy: 0.744\n",
      "train loss: 100%[**************************************************->]0.458\n",
      "141.78074939999988\n",
      "[epoch 19] train_loss: 0.786  test_accuracy: 0.683\n",
      "train loss: 100%[**************************************************->]0.617\n",
      "112.50367069999993\n",
      "[epoch 20] train_loss: 0.801  test_accuracy: 0.731\n",
      "train loss: 100%[**************************************************->]0.456\n",
      "121.04398980000042\n",
      "[epoch 21] train_loss: 0.788  test_accuracy: 0.685\n",
      "train loss: 100%[**************************************************->]1.219\n",
      "123.20341700000017\n",
      "[epoch 22] train_loss: 0.752  test_accuracy: 0.722\n",
      "train loss: 100%[**************************************************->]0.444\n",
      "120.78145320000021\n",
      "[epoch 23] train_loss: 0.740  test_accuracy: 0.754\n",
      "train loss: 100%[**************************************************->]0.034\n",
      "928.7975909999996\n",
      "[epoch 24] train_loss: 0.744  test_accuracy: 0.728\n",
      "train loss: 100%[**************************************************->]0.643\n",
      "109.4307958999998\n",
      "[epoch 25] train_loss: 0.728  test_accuracy: 0.757\n",
      "train loss: 100%[**************************************************->]1.433\n",
      "115.05852409999989\n",
      "[epoch 26] train_loss: 0.702  test_accuracy: 0.752\n",
      "train loss: 100%[**************************************************->]0.540\n",
      "114.73640299999988\n",
      "[epoch 27] train_loss: 0.683  test_accuracy: 0.728\n",
      "train loss: 100%[**************************************************->]0.364\n",
      "113.45736680000027\n",
      "[epoch 28] train_loss: 0.638  test_accuracy: 0.741\n",
      "train loss: 100%[**************************************************->]0.818\n",
      "116.62045729999954\n",
      "[epoch 29] train_loss: 0.682  test_accuracy: 0.754\n",
      "train loss: 100%[**************************************************->]0.340\n",
      "117.58210789999976\n",
      "[epoch 30] train_loss: 0.643  test_accuracy: 0.767\n",
      "train loss: 100%[**************************************************->]0.458\n",
      "117.88585260000036\n",
      "[epoch 31] train_loss: 0.669  test_accuracy: 0.748\n",
      "train loss: 100%[**************************************************->]0.797\n",
      "117.90885379999963\n",
      "[epoch 32] train_loss: 0.665  test_accuracy: 0.778\n",
      "train loss: 100%[**************************************************->]0.416\n",
      "9618.0803113\n",
      "[epoch 33] train_loss: 0.661  test_accuracy: 0.776\n",
      "train loss: 100%[**************************************************->]0.317\n",
      "108.62523909999982\n",
      "[epoch 34] train_loss: 0.648  test_accuracy: 0.776\n",
      "train loss: 100%[**************************************************->]0.154\n",
      "111.69089359999998\n",
      "[epoch 35] train_loss: 0.633  test_accuracy: 0.769\n",
      "train loss: 100%[**************************************************->]0.020\n",
      "111.87532830000055\n",
      "[epoch 36] train_loss: 0.620  test_accuracy: 0.787\n",
      "train loss: 100%[**************************************************->]0.283\n",
      "983.6691200999994\n",
      "[epoch 37] train_loss: 0.625  test_accuracy: 0.802\n",
      "train loss: 100%[**************************************************->]0.936\n",
      "108.1704353999994\n",
      "[epoch 38] train_loss: 0.598  test_accuracy: 0.785\n",
      "train loss: 100%[**************************************************->]0.310\n",
      "113.06581809999989\n",
      "[epoch 39] train_loss: 0.596  test_accuracy: 0.781\n",
      "train loss: 100%[**************************************************->]0.005\n",
      "113.64121979999982\n",
      "[epoch 40] train_loss: 0.607  test_accuracy: 0.787\n",
      "train loss: 100%[**************************************************->]0.453\n",
      "112.80235959999845\n",
      "[epoch 41] train_loss: 0.605  test_accuracy: 0.804\n",
      "train loss: 100%[**************************************************->]0.649\n",
      "114.35606959999859\n",
      "[epoch 42] train_loss: 0.615  test_accuracy: 0.783\n",
      "train loss: 100%[**************************************************->]0.294\n",
      "115.94548439999926\n",
      "[epoch 43] train_loss: 0.580  test_accuracy: 0.796\n",
      "train loss: 100%[**************************************************->]0.824\n",
      "116.89187190000303\n",
      "[epoch 44] train_loss: 0.511  test_accuracy: 0.796\n",
      "train loss: 100%[**************************************************->]0.047\n",
      "117.78629550000187\n",
      "[epoch 45] train_loss: 0.569  test_accuracy: 0.796\n",
      "train loss: 100%[**************************************************->]0.799\n",
      "117.94742659999974\n",
      "[epoch 46] train_loss: 0.564  test_accuracy: 0.796\n",
      "train loss: 100%[**************************************************->]0.300\n",
      "118.40564520000044\n",
      "[epoch 47] train_loss: 0.568  test_accuracy: 0.807\n",
      "train loss: 100%[**************************************************->]0.457\n",
      "118.65243720000217\n",
      "[epoch 48] train_loss: 0.541  test_accuracy: 0.819\n",
      "train loss: 100%[**************************************************->]1.341\n",
      "119.00615050000124\n",
      "[epoch 49] train_loss: 0.535  test_accuracy: 0.772\n",
      "train loss: 100%[**************************************************->]0.043\n",
      "118.97241249999934\n",
      "[epoch 50] train_loss: 0.511  test_accuracy: 0.819\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(50):\n",
    "    # train\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    t1 = time.perf_counter()\n",
    "    for step, data in enumerate(train_loader, start=0):\n",
    "        images, labels = data\n",
    "        #print(labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images.to(device))\n",
    "        loss = loss_function(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # print train process\n",
    "        rate = (step + 1) / len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\"\\rtrain loss: {:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss), end=\"\")\n",
    "    print()\n",
    "    print(time.perf_counter()-t1)\n",
    "\n",
    "    # validate\n",
    "    net.eval()  \n",
    "    acc = 0.0  # accumulate accurate number / epoch\n",
    "    with torch.no_grad():\n",
    "        for val_data in validate_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            outputs = net(val_images.to(device))\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            acc += (predict_y == val_labels.to(device)).sum().item()\n",
    "        val_accurate = acc / val_num\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "        print('[epoch %d] train_loss: %.3f  test_accuracy: %.3f' %(epoch + 1, running_loss / step, val_accurate))\n",
    "    losses.append(running_loss/step)\n",
    "    accs.append(val_accurate)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 48, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=4608, out_features=2048, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=2048, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Super_AlexNet(kernels, num_classes=9, init_weights=False)\n",
    "save_path = \"Super_AlexNet.pth\"\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
