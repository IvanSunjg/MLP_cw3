{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from model import AlexNet\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#device : GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "    \"val\": transforms.Compose([transforms.Resize((224, 224)),  # cannot 224, must (224, 224)\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
    "data_root = os.getcwd()\n",
    "image_path = data_root + \"/data/\"  # flower data set path\n",
    "train_dataset = datasets.ImageFolder(root=image_path + \"/train\",\n",
    "                                     transform=data_transform[\"train\"])\n",
    "train_num = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_list = train_dataset.class_to_idx\n",
    "cla_dict = dict((val, key) for key, val in cd_list.items())\n",
    "# write dict into json file\n",
    "json_str = json.dumps(cla_dict, indent=4)\n",
    "with open('class_indices.json', 'w') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) <class 'torch.Tensor'>\n",
      "tensor(8) 8 <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(root=image_path + \"/val\",\n",
    "                                        transform=data_transform[\"val\"])\n",
    "val_num = len(validate_dataset)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                              batch_size=batch_size, shuffle=True,\n",
    "                                              num_workers=0)\n",
    "\n",
    "test_data_iter = iter(validate_loader)\n",
    "test_image, test_label = test_data_iter.next()\n",
    "print(test_image[0].size(),type(test_image[0]))\n",
    "print(test_label[0],test_label[0].item(),type(test_label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AlexNet(num_classes=9, init_weights=False)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0002)\n",
    "\n",
    "save_path = './AlexNet.pth'\n",
    "\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 100%[**************************************************->]1.920\n",
      "9.790819900000002\n",
      "[epoch 1] train_loss: 1.925  test_accuracy: 0.322\n",
      "train loss: 100%[**************************************************->]1.356\n",
      "10.40760569999999\n",
      "[epoch 2] train_loss: 1.541  test_accuracy: 0.443\n",
      "train loss: 100%[**************************************************->]0.987\n",
      "10.433250299999997\n",
      "[epoch 3] train_loss: 1.366  test_accuracy: 0.448\n",
      "train loss: 100%[**************************************************->]1.518\n",
      "10.460197699999995\n",
      "[epoch 4] train_loss: 1.300  test_accuracy: 0.511\n",
      "train loss: 100%[**************************************************->]1.275\n",
      "10.516287599999998\n",
      "[epoch 5] train_loss: 1.275  test_accuracy: 0.500\n",
      "train loss: 100%[**************************************************->]1.184\n",
      "10.376902299999998\n",
      "[epoch 6] train_loss: 1.179  test_accuracy: 0.524\n",
      "train loss: 100%[**************************************************->]2.732\n",
      "10.286304400000006\n",
      "[epoch 7] train_loss: 1.151  test_accuracy: 0.550\n",
      "train loss: 100%[**************************************************->]1.629\n",
      "10.186280100000005\n",
      "[epoch 8] train_loss: 1.104  test_accuracy: 0.530\n",
      "train loss: 100%[**************************************************->]1.060\n",
      "10.456088300000005\n",
      "[epoch 9] train_loss: 1.073  test_accuracy: 0.569\n",
      "train loss: 100%[**************************************************->]1.440\n",
      "10.756440300000008\n",
      "[epoch 10] train_loss: 1.020  test_accuracy: 0.520\n",
      "train loss: 100%[**************************************************->]0.820\n",
      "10.072331699999978\n",
      "[epoch 11] train_loss: 1.006  test_accuracy: 0.600\n",
      "train loss: 100%[**************************************************->]1.829\n",
      "10.381271699999985\n",
      "[epoch 12] train_loss: 0.972  test_accuracy: 0.657\n",
      "train loss: 100%[**************************************************->]0.989\n",
      "10.130657899999989\n",
      "[epoch 13] train_loss: 0.939  test_accuracy: 0.631\n",
      "train loss: 100%[**************************************************->]1.009\n",
      "10.341213799999991\n",
      "[epoch 14] train_loss: 0.916  test_accuracy: 0.644\n",
      "train loss: 100%[**************************************************->]1.613\n",
      "9.894933900000012\n",
      "[epoch 15] train_loss: 0.897  test_accuracy: 0.681\n",
      "train loss: 100%[**************************************************->]1.079\n",
      "9.844684400000006\n",
      "[epoch 16] train_loss: 0.892  test_accuracy: 0.659\n",
      "train loss: 100%[**************************************************->]0.791\n",
      "10.076347800000008\n",
      "[epoch 17] train_loss: 0.846  test_accuracy: 0.698\n",
      "train loss: 100%[**************************************************->]1.081\n",
      "10.500474800000006\n",
      "[epoch 18] train_loss: 0.863  test_accuracy: 0.628\n",
      "train loss: 100%[**************************************************->]0.730\n",
      "10.382468700000004\n",
      "[epoch 19] train_loss: 0.848  test_accuracy: 0.715\n",
      "train loss: 100%[**************************************************->]0.986\n",
      "10.034162100000003\n",
      "[epoch 20] train_loss: 0.786  test_accuracy: 0.730\n",
      "train loss: 100%[**************************************************->]0.480\n",
      "10.149635199999977\n",
      "[epoch 21] train_loss: 0.832  test_accuracy: 0.683\n",
      "train loss: 100%[**************************************************->]0.718\n",
      "10.528232100000025\n",
      "[epoch 22] train_loss: 0.779  test_accuracy: 0.709\n",
      "train loss: 100%[**************************************************->]0.867\n",
      "9.870986099999982\n",
      "[epoch 23] train_loss: 0.769  test_accuracy: 0.689\n",
      "train loss: 100%[**************************************************->]0.510\n",
      "9.7897332\n",
      "[epoch 24] train_loss: 0.743  test_accuracy: 0.730\n",
      "train loss: 100%[**************************************************->]0.300\n",
      "9.731396800000027\n",
      "[epoch 25] train_loss: 0.727  test_accuracy: 0.696\n",
      "train loss: 100%[**************************************************->]1.804\n",
      "9.824427599999979\n",
      "[epoch 26] train_loss: 0.722  test_accuracy: 0.741\n",
      "train loss: 100%[**************************************************->]1.089\n",
      "9.91425989999999\n",
      "[epoch 27] train_loss: 0.708  test_accuracy: 0.704\n",
      "train loss: 100%[**************************************************->]0.225\n",
      "10.219942800000013\n",
      "[epoch 28] train_loss: 0.706  test_accuracy: 0.733\n",
      "train loss: 100%[**************************************************->]1.006\n",
      "10.025236399999983\n",
      "[epoch 29] train_loss: 0.699  test_accuracy: 0.730\n",
      "train loss: 100%[**************************************************->]0.094\n",
      "9.87418359999998\n",
      "[epoch 30] train_loss: 0.669  test_accuracy: 0.724\n",
      "train loss: 100%[**************************************************->]0.152\n",
      "9.786620199999959\n",
      "[epoch 31] train_loss: 0.651  test_accuracy: 0.756\n",
      "train loss: 100%[**************************************************->]0.909\n",
      "9.868223600000022\n",
      "[epoch 32] train_loss: 0.701  test_accuracy: 0.724\n",
      "train loss: 100%[**************************************************->]0.525\n",
      "9.798440399999947\n",
      "[epoch 33] train_loss: 0.675  test_accuracy: 0.748\n",
      "train loss: 100%[**************************************************->]0.137\n",
      "9.842148099999974\n",
      "[epoch 34] train_loss: 0.671  test_accuracy: 0.791\n",
      "train loss: 100%[**************************************************->]0.041\n",
      "9.98967110000001\n",
      "[epoch 35] train_loss: 0.606  test_accuracy: 0.743\n",
      "train loss: 100%[**************************************************->]0.107\n",
      "10.77874509999998\n",
      "[epoch 36] train_loss: 0.632  test_accuracy: 0.748\n",
      "train loss: 100%[**************************************************->]0.366\n",
      "10.492020400000001\n",
      "[epoch 37] train_loss: 0.616  test_accuracy: 0.724\n",
      "train loss: 100%[**************************************************->]0.223\n",
      "10.35128309999999\n",
      "[epoch 38] train_loss: 0.613  test_accuracy: 0.772\n",
      "train loss: 100%[**************************************************->]1.104\n",
      "10.382088300000078\n",
      "[epoch 39] train_loss: 0.611  test_accuracy: 0.793\n",
      "train loss: 100%[**************************************************->]1.190\n",
      "10.532225599999947\n",
      "[epoch 40] train_loss: 0.577  test_accuracy: 0.757\n",
      "train loss: 100%[**************************************************->]0.014\n",
      "10.666488400000048\n",
      "[epoch 41] train_loss: 0.595  test_accuracy: 0.783\n",
      "train loss: 100%[**************************************************->]0.224\n",
      "10.058017199999995\n",
      "[epoch 42] train_loss: 0.556  test_accuracy: 0.791\n",
      "train loss: 100%[**************************************************->]1.058\n",
      "10.359776300000021\n",
      "[epoch 43] train_loss: 0.568  test_accuracy: 0.754\n",
      "train loss: 100%[**************************************************->]0.585\n",
      "10.060751700000083\n",
      "[epoch 44] train_loss: 0.540  test_accuracy: 0.778\n",
      "train loss: 100%[**************************************************->]1.407\n",
      "10.248995400000013\n",
      "[epoch 45] train_loss: 0.569  test_accuracy: 0.789\n",
      "train loss: 100%[**************************************************->]0.187\n",
      "11.68308049999996\n",
      "[epoch 46] train_loss: 0.552  test_accuracy: 0.793\n",
      "train loss: 100%[**************************************************->]0.327\n",
      "13.883809499999984\n",
      "[epoch 47] train_loss: 0.532  test_accuracy: 0.770\n",
      "train loss: 100%[**************************************************->]0.021\n",
      "13.104084199999988\n",
      "[epoch 48] train_loss: 0.549  test_accuracy: 0.781\n",
      "train loss: 100%[**************************************************->]0.829\n",
      "13.532026799999926\n",
      "[epoch 49] train_loss: 0.514  test_accuracy: 0.800\n",
      "train loss: 100%[**************************************************->]1.571\n",
      "13.42482009999992\n",
      "[epoch 50] train_loss: 0.565  test_accuracy: 0.781\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(50):\n",
    "    # train\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    t1 = time.perf_counter()\n",
    "    for step, data in enumerate(train_loader, start=0):\n",
    "        images, labels = data\n",
    "        #print(labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images.to(device))\n",
    "        loss = loss_function(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # print train process\n",
    "        rate = (step + 1) / len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\"\\rtrain loss: {:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss), end=\"\")\n",
    "    print()\n",
    "    print(time.perf_counter()-t1)\n",
    "\n",
    "    # validate\n",
    "    net.eval()  \n",
    "    acc = 0.0  # accumulate accurate number / epoch\n",
    "    with torch.no_grad():\n",
    "        for val_data in validate_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            outputs = net(val_images.to(device))\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            acc += (predict_y == val_labels.to(device)).sum().item()\n",
    "        val_accurate = acc / val_num\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "        print('[epoch %d] train_loss: %.3f  test_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / step, val_accurate))\n",
    "    losses.append(running_loss/step)\n",
    "    accs.append(val_accurate)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 48, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=4608, out_features=2048, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=2048, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AlexNet(num_classes=9, init_weights=False)\n",
    "save_path = \"AlexNet0.6.pth\"\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" with torch.no_grad():\\n    for test_data in test_loader:\\n        val_images, val_labels = val_data\\n        outputs = net(val_images.to(device))\\n        predict_y = torch.max(outputs, dim=1)[1]\\n        acc += (predict_y == val_labels.to(device)).sum().item()\\n    val_accurate = acc / val_num\\n    if val_accurate > best_acc:\\n        best_acc = val_accurate\\n        torch.save(net.state_dict(), save_path)\\n    print('[epoch %d] train_loss: %.3f  test_accuracy: %.3f' %\\n            (epoch + 1, running_loss / step, val_accurate)) \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        val_images, val_labels = val_data\n",
    "        outputs = net(val_images.to(device))\n",
    "        predict_y = torch.max(outputs, dim=1)[1]\n",
    "        acc += (predict_y == val_labels.to(device)).sum().item()\n",
    "    val_accurate = acc / val_num\n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "    print('[epoch %d] train_loss: %.3f  test_accuracy: %.3f' %\n",
    "            (epoch + 1, running_loss / step, val_accurate)) \"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
